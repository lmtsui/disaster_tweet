{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import torch\n",
    "\n",
    "#read data from csv\n",
    "df=pd.read_csv('data/train.csv')\n",
    "input_text = df.text.values\n",
    "keywords = df.keyword.values\n",
    "input_str = [' '.join([str(keywords[i]),sen]) for i,sen in enumerate(input_text)]\n",
    "input_labels = df.target.values\n",
    "\n",
    "from transformers import BertTokenizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from utils import padding\n",
    "\n",
    "#preprocessing\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased',do_lowercase=True)\n",
    "input_ids = [tokenizer.encode(sen,add_special_tokens=True) for sen in input_str]\n",
    "input_ids = padding(input_ids)\n",
    "att_masks = [[int(ids>0) for ids in sen] for sen in input_ids]\n",
    "train_ids, val_ids, train_labels, val_labels, train_masks, val_masks = train_test_split(input_ids, input_labels, att_masks, test_size=0.1, random_state = 42)\n",
    "\n",
    "#convert to pytorch tensors\n",
    "train_ids = torch.tensor(train_ids)\n",
    "val_ids = torch.tensor(val_ids)\n",
    "train_labels = torch.tensor(train_labels)\n",
    "val_labels = torch.tensor(val_labels)\n",
    "train_masks = torch.tensor(train_masks)\n",
    "val_masks = torch.tensor(val_masks)\n",
    "\n",
    "from torch.utils.data import DataLoader,SequentialSampler,RandomSampler,TensorDataset\n",
    "\n",
    "#wrap into datasets\n",
    "train_data = TensorDataset(train_ids, train_masks, train_labels)\n",
    "val_data = TensorDataset(val_ids, val_masks, val_labels)\n",
    "\n",
    "batch_size = 16\n",
    "\n",
    "#prepare dataloaders\n",
    "train_sampler = RandomSampler(train_data)\n",
    "val_sampler = SequentialSampler(val_data)\n",
    "train_loader = DataLoader(train_data,sampler = train_sampler, batch_size = batch_size)\n",
    "val_loader = DataLoader(val_data, sampler = val_sampler, batch_size = batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from model import BertPooled, BertClassifier\n",
    "\n",
    "#initiate bert model\n",
    "modelpooled = BertPooled.from_pretrained(\n",
    "    \"bert-base-uncased\", # Use the 12-layer BERT model, with an uncased vocab.\n",
    "    num_labels = 2,\n",
    "    output_attentions = False, \n",
    "    output_hidden_states = False, \n",
    ")\n",
    "#classifier has three linear layers. see model.py\n",
    "classifier = BertClassifier.from_pretrained(\n",
    "    \"bert-base-uncased\", # Use the 12-layer BERT model, with an uncased vocab.\n",
    "    num_labels = 2,\n",
    "    output_attentions = False, \n",
    "    output_hidden_states = False, \n",
    ")\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    modelpooled.cuda()\n",
    "    classifier.cuda()\n",
    "    device=torch.device('cuda')\n",
    "else:\n",
    "    device=torch.device('cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "109482240"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#total number of parameters in model\n",
    "sum([p.numel() for p in modelpooled.parameters()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AdamW,get_linear_schedule_with_warmup\n",
    "epochs = 4\n",
    "total_steps = len(train_loader) * epochs\n",
    "optimizer = AdamW(list(modelpooled.parameters())+list(classifier.parameters()),lr = 2e-5, eps = 1e-8)\n",
    "# scheduler = get_linear_schedule_with_warmup(optimizer,num_warmup_steps = 0,num_training_steps = total_steps)\n",
    "scheduler =  torch.optim.lr_scheduler.ExponentialLR(optimizer,gamma=0.3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils import format_time\n",
    "\n",
    "# Function to calculate the accuracy of our predictions vs labels\n",
    "def flat_accuracy(preds, labels):\n",
    "    pred_flat = np.argmax(preds, axis=1).flatten()\n",
    "    labels_flat = labels.flatten()\n",
    "    return np.sum(pred_flat == labels_flat) / len(labels_flat)\n",
    "\n",
    "def validate(modelpooled,classifier, val_loader, device):\n",
    "    #Validation\n",
    "    print(\"\")\n",
    "    print(\"Running Validation...\")\n",
    "\n",
    "    t0 = time.time()\n",
    "\n",
    "    modelpooled.eval()\n",
    "    classifier.eval()\n",
    "\n",
    "    # Tracking variables \n",
    "    eval_loss, eval_accuracy = 0, 0\n",
    "    nb_eval_steps, nb_eval_examples = 0, 0\n",
    "\n",
    "    # Evaluate data for one epoch\n",
    "    for batch in val_loader:\n",
    "\n",
    "        # Add batch to GPU\n",
    "        batch = tuple(t.to(device) for t in batch)\n",
    "        b_input_ids, b_input_mask, b_labels = batch\n",
    "\n",
    "        # Telling the model not to compute or store gradients, saving memory and\n",
    "        # speeding up validation\n",
    "        with torch.no_grad():        \n",
    "\n",
    "        # Forward pass, calculate logit predictions.\n",
    "        # This will return the logits rather than the loss because we have\n",
    "        # not provided labels.\n",
    "        # token_type_ids is the same as the \"segment ids\", which \n",
    "        # differentiates sentence 1 and 2 in 2-sentence tasks.\n",
    "        # The documentation for this `model` function is here: \n",
    "        # https://huggingface.co/transformers/v2.2.0/model_doc/bert.html#transformers.BertForSequenceClassification\n",
    "            outputs = modelpooled(b_input_ids, \n",
    "                                  token_type_ids=None,\n",
    "                                  attention_mask=b_input_mask)[0]\n",
    "            outputs = classifier(outputs)\n",
    "\n",
    "        # Get the \"logits\" output by the model. The \"logits\" are the output\n",
    "        # values prior to applying an activation function like the softmax.\n",
    "        logits = outputs[0]\n",
    "\n",
    "        # Move logits and labels to CPU\n",
    "        logits = logits.detach().cpu().numpy()\n",
    "        label_ids = b_labels.to('cpu').numpy()\n",
    "\n",
    "        # Calculate the accuracy for this batch of test sentences.\n",
    "        tmp_eval_accuracy = flat_accuracy(logits, label_ids)\n",
    "\n",
    "        # Accumulate the total accuracy.\n",
    "        eval_accuracy += tmp_eval_accuracy\n",
    "\n",
    "        # Track the number of batches\n",
    "        nb_eval_steps += 1\n",
    "\n",
    "    # Report the final accuracy for this validation run.\n",
    "    print(\"  Accuracy: {0:.2f}\".format(eval_accuracy/nb_eval_steps))\n",
    "    print(\"  Validation took: {:}\".format(format_time(time.time() - t0)))\n",
    "    \n",
    "    return eval_accuracy/nb_eval_steps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "======== Epoch 1 / 4 ========\n",
      "Training...\n",
      "  Batch    40  of    429.    Elapsed: 0:00:21.\n",
      "  Batch    80  of    429.    Elapsed: 0:00:42.\n",
      "  Batch   120  of    429.    Elapsed: 0:01:03.\n",
      "  Batch   160  of    429.    Elapsed: 0:01:25.\n",
      "  Batch   200  of    429.    Elapsed: 0:01:46.\n",
      "  Batch   240  of    429.    Elapsed: 0:02:08.\n",
      "  Batch   280  of    429.    Elapsed: 0:02:30.\n",
      "  Batch   320  of    429.    Elapsed: 0:02:51.\n",
      "  Batch   360  of    429.    Elapsed: 0:03:13.\n",
      "  Batch   400  of    429.    Elapsed: 0:03:35.\n",
      "\n",
      "  Average training loss: 0.51\n",
      "  Training epoch took: 0:03:51\n",
      "\n",
      "Running Validation...\n",
      "  Accuracy: 0.83\n",
      "  Validation took: 0:00:06\n",
      "\n",
      "======== Epoch 2 / 4 ========\n",
      "Training...\n",
      "  Batch    40  of    429.    Elapsed: 0:00:22.\n",
      "  Batch    80  of    429.    Elapsed: 0:00:44.\n",
      "  Batch   120  of    429.    Elapsed: 0:01:06.\n",
      "  Batch   160  of    429.    Elapsed: 0:01:28.\n",
      "  Batch   200  of    429.    Elapsed: 0:01:50.\n",
      "  Batch   240  of    429.    Elapsed: 0:02:12.\n",
      "  Batch   280  of    429.    Elapsed: 0:02:34.\n",
      "  Batch   320  of    429.    Elapsed: 0:02:56.\n",
      "  Batch   360  of    429.    Elapsed: 0:03:18.\n",
      "  Batch   400  of    429.    Elapsed: 0:03:40.\n",
      "\n",
      "  Average training loss: 0.37\n",
      "  Training epoch took: 0:03:55\n",
      "\n",
      "Running Validation...\n",
      "  Accuracy: 0.82\n",
      "  Validation took: 0:00:06\n",
      "\n",
      "======== Epoch 3 / 4 ========\n",
      "Training...\n",
      "  Batch    40  of    429.    Elapsed: 0:00:22.\n",
      "  Batch    80  of    429.    Elapsed: 0:00:44.\n",
      "  Batch   120  of    429.    Elapsed: 0:01:06.\n",
      "  Batch   160  of    429.    Elapsed: 0:01:28.\n",
      "  Batch   200  of    429.    Elapsed: 0:01:50.\n",
      "  Batch   240  of    429.    Elapsed: 0:02:12.\n",
      "  Batch   280  of    429.    Elapsed: 0:02:35.\n",
      "  Batch   320  of    429.    Elapsed: 0:02:57.\n",
      "  Batch   360  of    429.    Elapsed: 0:03:19.\n",
      "  Batch   400  of    429.    Elapsed: 0:03:41.\n",
      "\n",
      "  Average training loss: 0.32\n",
      "  Training epoch took: 0:03:57\n",
      "\n",
      "Running Validation...\n",
      "  Accuracy: 0.84\n",
      "  Validation took: 0:00:06\n",
      "\n",
      "======== Epoch 4 / 4 ========\n",
      "Training...\n",
      "  Batch    40  of    429.    Elapsed: 0:00:22.\n",
      "  Batch    80  of    429.    Elapsed: 0:00:44.\n",
      "  Batch   120  of    429.    Elapsed: 0:01:06.\n",
      "  Batch   160  of    429.    Elapsed: 0:01:28.\n",
      "  Batch   200  of    429.    Elapsed: 0:01:50.\n",
      "  Batch   240  of    429.    Elapsed: 0:02:12.\n",
      "  Batch   280  of    429.    Elapsed: 0:02:35.\n",
      "  Batch   320  of    429.    Elapsed: 0:02:57.\n",
      "  Batch   360  of    429.    Elapsed: 0:03:19.\n",
      "  Batch   400  of    429.    Elapsed: 0:03:41.\n",
      "\n",
      "  Average training loss: 0.31\n",
      "  Training epoch took: 0:03:57\n",
      "\n",
      "Running Validation...\n",
      "  Accuracy: 0.84\n",
      "  Validation took: 0:00:06\n",
      "\n",
      "Training complete!\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "import numpy as np\n",
    "import time\n",
    "\n",
    "# This training code is based on the `run_glue.py` script here:\n",
    "# https://github.com/huggingface/transformers/blob/5bfcd0485ece086ebcbed2d008813037968a9e58/examples/run_glue.py#L128\n",
    "\n",
    "\n",
    "# Set the seed value all over the place to make this reproducible.\n",
    "seed_val = 42\n",
    "\n",
    "random.seed(seed_val)\n",
    "np.random.seed(seed_val)\n",
    "torch.manual_seed(seed_val)\n",
    "torch.cuda.manual_seed_all(seed_val)\n",
    "\n",
    "# Store the average loss after each epoch so we can plot them.\n",
    "loss_values = []\n",
    "\n",
    "for epoch_i in range(0, epochs):\n",
    "    # Perform one full pass over the training set.\n",
    "    print(\"\")\n",
    "    print('======== Epoch {:} / {:} ========'.format(epoch_i + 1, epochs))\n",
    "    print('Training...')\n",
    "\n",
    "    t0 = time.time()\n",
    "    total_loss = 0\n",
    "    modelpooled.train()\n",
    "    classifier.train()\n",
    "\n",
    "    for step, batch in enumerate(train_loader):\n",
    "\n",
    "        if step % 40 == 0 and not step == 0:\n",
    "            # Calculate elapsed time in minutes.\n",
    "            elapsed = format_time(time.time() - t0)\n",
    "            print('  Batch {:>5,}  of  {:>5,}.    Elapsed: {:}.'.format(step, len(train_loader), elapsed))\n",
    "        # `batch` contains three pytorch tensors:\n",
    "        #   [0]: input ids \n",
    "        #   [1]: attention masks\n",
    "        #   [2]: labels \n",
    "        b_input_ids = batch[0].to(device)\n",
    "        b_input_mask = batch[1].to(device)\n",
    "        b_labels = batch[2].to(device)\n",
    "        \n",
    "#         model.zero_grad()  \n",
    "        modelpooled.zero_grad()\n",
    "        classifier.zero_grad()\n",
    "\n",
    "        # This will return the loss (rather than the model output) because we\n",
    "        # have provided the `labels`.\n",
    "        # The documentation for this `model` function is here: \n",
    "        # https://huggingface.co/transformers/v2.2.0/model_doc/bert.html#transformers.BertForSequenceClassification\n",
    "        \n",
    "        outputs = modelpooled(b_input_ids, \n",
    "            token_type_ids=None, \n",
    "            attention_mask=b_input_mask)[0]\n",
    "        outputs = classifier(outputs,labels=b_labels)\n",
    "\n",
    "\n",
    "#         outputs = model(b_input_ids, \n",
    "#                     token_type_ids=None, \n",
    "#                     attention_mask=b_input_mask, \n",
    "#                     labels=b_labels)\n",
    "        \n",
    "        # The call to `model` always returns a tuple, so we need to pull the \n",
    "        # loss value out of the tuple.\n",
    "        loss = outputs[0]\n",
    "\n",
    "        # Accumulate the training loss over all of the batches so that we can\n",
    "        # calculate the average loss at the end. `loss` is a Tensor containing a\n",
    "        # single value; the `.item()` function just returns the Python value \n",
    "        # from the tensor.\n",
    "        total_loss += loss.item()\n",
    "\n",
    "        loss.backward()\n",
    "\n",
    "        torch.nn.utils.clip_grad_norm_(modelpooled.parameters(), 1.0)\n",
    "        torch.nn.utils.clip_grad_norm_(classifier.parameters(), 1.0)\n",
    "\n",
    "        optimizer.step()\n",
    "#         scheduler.step()\n",
    "\n",
    "    avg_train_loss = total_loss / len(train_loader)            \n",
    "\n",
    "    loss_values.append(avg_train_loss)\n",
    "\n",
    "    print(\"\")\n",
    "    print(\"  Average training loss: {0:.2f}\".format(avg_train_loss))\n",
    "    print(\"  Training epoch took: {:}\".format(format_time(time.time() - t0)))\n",
    "    \n",
    "    validate(modelpooled,classifier, val_loader, device)\n",
    "    scheduler.step()\n",
    "\n",
    "print(\"\")\n",
    "print(\"Training complete!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#save\n",
    "modelpooled.save_pretrained('save/modelpooled/')\n",
    "classifier.save_pretrained('save/classifier/')\n",
    "tokenizer.save_pretrained('save/')\n",
    "torch.save(optimizer.state_dict(),'save/optim.bin')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#load\n",
    "modelpooled = BertPooled.from_pretrained('save/modelpooled/').to(device)\n",
    "classifier = BertClassifier.from_pretrained('save/classifier/').to(device)\n",
    "tokenizer = BertTokenizer.from_pretrained('save/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "#load optimizer\n",
    "checkpoint= torch.load('save/optim.bin')\n",
    "optimizer.load_state_dict(checkpoint)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#process training data with the trained bert\n",
    "pretrain_sampler = SequentialSampler(train_data)\n",
    "pretrain_loader = DataLoader(train_data,sampler = pretrain_sampler, batch_size = batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "973a8f1c889846cab5e081fe23dd73da",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=429), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm_notebook\n",
    "bert_output = []\n",
    "labels=[]\n",
    "for batch in tqdm_notebook(pretrain_loader):\n",
    "\n",
    "        # Add batch to GPU\n",
    "        batch = tuple(t.to(device) for t in batch)\n",
    "        b_input_ids, b_input_mask, b_labels = batch\n",
    "\n",
    "        # Telling the model not to compute or store gradients, saving memory and\n",
    "        # speeding up validation\n",
    "        with torch.no_grad():        \n",
    "\n",
    "        # Forward pass, calculate logit predictions.\n",
    "        # This will return the logits rather than the loss because we have\n",
    "        # not provided labels.\n",
    "        # token_type_ids is the same as the \"segment ids\", which \n",
    "        # differentiates sentence 1 and 2 in 2-sentence tasks.\n",
    "        # The documentation for this `model` function is here: \n",
    "        # https://huggingface.co/transformers/v2.2.0/model_doc/bert.html#transformers.BertForSequenceClassification\n",
    "            outputs = modelpooled(b_input_ids, \n",
    "                                  token_type_ids=None,\n",
    "                                  attention_mask=b_input_mask)[0]\n",
    "            bert_output.append(outputs.cpu().numpy())\n",
    "            labels.append(b_labels.cpu().numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#convert to numpy tensors\n",
    "bert_np = np.concatenate(bert_output)\n",
    "labels_np = np.concatenate(labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(6851, 768)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bert_np.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(6851,)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labels_np.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.save('bert_np',bert_np)\n",
    "np.save('labels_np',labels_np)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "39e4f961c8374ee7977f14b9c8e85744",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=48), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "#process validation data with the trained bert\n",
    "bert_output = []\n",
    "labels=[]\n",
    "for batch in tqdm_notebook(val_loader):\n",
    "\n",
    "        # Add batch to GPU\n",
    "        batch = tuple(t.to(device) for t in batch)\n",
    "        b_input_ids, b_input_mask, b_labels = batch\n",
    "\n",
    "        # Telling the model not to compute or store gradients, saving memory and\n",
    "        # speeding up validation\n",
    "        with torch.no_grad():        \n",
    "\n",
    "        # Forward pass, calculate logit predictions.\n",
    "        # This will return the logits rather than the loss because we have\n",
    "        # not provided labels.\n",
    "        # token_type_ids is the same as the \"segment ids\", which \n",
    "        # differentiates sentence 1 and 2 in 2-sentence tasks.\n",
    "        # The documentation for this `model` function is here: \n",
    "        # https://huggingface.co/transformers/v2.2.0/model_doc/bert.html#transformers.BertForSequenceClassification\n",
    "            outputs = modelpooled(b_input_ids, \n",
    "                                  token_type_ids=None,\n",
    "                                  attention_mask=b_input_mask)[0]\n",
    "            bert_output.append(outputs.cpu().numpy())\n",
    "            labels.append(b_labels.cpu().numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "#convert to numpy tensors\n",
    "val_bert_np = np.concatenate(bert_output)\n",
    "val_labels_np = np.concatenate(labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.save('val_bert_np',val_bert_np)\n",
    "np.save('val_labels_np',val_labels_np)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "XGBClassifier(base_score=0.5, booster='gbtree', colsample_bylevel=1,\n",
       "              colsample_bynode=1, colsample_bytree=1, gamma=0,\n",
       "              learning_rate=0.1, max_delta_step=0, max_depth=3,\n",
       "              min_child_weight=1, missing=None, n_estimators=100, n_jobs=1,\n",
       "              nthread=None, objective='binary:logistic', random_state=0,\n",
       "              reg_alpha=0, reg_lambda=1, scale_pos_weight=1, seed=None,\n",
       "              silent=None, subsample=1, verbosity=1)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#use xgboost to classify output of BERT\n",
    "from xgboost import XGBClassifier\n",
    "model = XGBClassifier()\n",
    "model.fit(bert_np,labels_np)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.84251968503937"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#get accuracy of 0.84 again\n",
    "y_pred = model.predict(val_bert_np)\n",
    "predictions = [round(value) for value in y_pred]\n",
    "from sklearn.metrics import accuracy_score\n",
    "accuracy_score(list(val_labels_np),predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
